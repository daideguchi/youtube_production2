# LLM Router Configuration
# Defines providers, models, tiers, and task mappings.
#
# 2025-12-13:
# Default routing updated to OpenRouter low-cost chain (Rank A/B/C style).
# Azure remains optional/registered but is not selected by default tiers.

providers:
  azure:
    env_api_key: AZURE_OPENAI_API_KEY
    env_endpoint: AZURE_OPENAI_ENDPOINT
    default_api_version: "2024-10-01-preview"
  
  gemini:
    env_api_key: GEMINI_API_KEY
  
  openrouter:
    env_api_key: OPENROUTER_API_KEY
    base_url: "https://openrouter.ai/api/v1"

  fireworks:
    # Fireworks is OpenAI-compatible; used primarily for script writing to avoid OpenRouter quota.
    env_api_key: FIREWORKS_SCRIPT
    base_url: "https://api.fireworks.ai/inference/v1"

models:
  # --- Azure Models ---
  azure_gpt5_mini:
    provider: azure
    deployment: gpt-5-mini
    api_version: "2025-04-01-preview"
    capabilities:
      mode: chat
      reasoning: true
      json_mode: true
      max_completion_tokens: 128000
  
  # --- Gemini Models (Image Only) ---
  gemini_2_5_flash_image:
    provider: gemini
    model_name: gemini-2.5-flash-image
    capabilities:
      mode: image_generation
      vision: false
      json_mode: false

  # --- Script Writing (DeepSeek) ---
  # DeepSeek v3.2 exp: keep the OpenRouter model id as the "logical" name for compatibility.
  # LLMRouter maps it to Fireworks' OpenAI-compatible model id at runtime.
  or_deepseek_v3_2_exp:
    provider: fireworks
    model_name: "deepseek/deepseek-v3.2-exp"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      # Fireworks allows long outputs with streaming; keep this comfortably high to avoid
      # finish_reason=length loops on whole-script rewrites.
      max_tokens: 65535

  # --- Script Writing (Fireworks alternatives; opt-in via LLM_FORCE_MODELS / --llm-model) ---
  # NOTE: These are NOT used by default tiers/tasks to avoid style/quality drift.
  fw_glm_4p7:
    provider: fireworks
    model_name: "accounts/fireworks/models/glm-4p7"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 65535

  fw_kimi_k2_thinking:
    provider: fireworks
    model_name: "accounts/fireworks/models/kimi-k2-thinking"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 65535

  # Mistral-family (Mixtral) baseline — not a reasoning model; use for comparison only.
  fw_mixtral_8x22b_instruct:
    provider: fireworks
    model_name: "accounts/fireworks/models/mixtral-8x22b-instruct"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 65535

  # --- OpenRouter Models ---

  # Open reasoning (thinking) model — preferred fallback for script-writing when V3.2 is unavailable.
  or_kimi_k2_thinking:
    provider: openrouter
    model_name: "moonshotai/kimi-k2-thinking"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 65535

  # Premium (very expensive) — use ONLY for script_master_plan (blueprint) when explicitly enabled.
  # OpenRouter model id verified via /api/v1/models: anthropic/claude-opus-4.5
  or_claude_opus_4_5:
    provider: openrouter
    model_name: "anthropic/claude-opus-4.5"
    capabilities:
      mode: chat
      # Allow temperature; control cost via task defaults + master-plan env guards.
      reasoning: false
      json_mode: false
      max_tokens: 32000

  # or_o3_mini:
  #   provider: openrouter
  #   model_name: "openai/o3-mini"
  #   capabilities:
  #     mode: chat
  #     reasoning: true
  #     json_mode: true
  #     max_tokens: 16384

  or_qwen_2_5_7b_instruct:
    provider: openrouter
    model_name: "qwen/qwen-2.5-7b-instruct"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: false
      max_tokens: 8192

  or_hunyuan_a13b_instruct:
    provider: openrouter
    model_name: "tencent/hunyuan-a13b-instruct"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 16384

  # Vision (image + text → text) — for thumbnail captioning
  or_qwen_2_5_vl_72b_instruct:
    provider: openrouter
    model_name: "qwen/qwen2.5-vl-72b-instruct"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 8192

  or_gemma_3n_e2b_it_free:
    provider: openrouter
    model_name: "google/gemma-3n-e2b-it:free"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 2048

  or_mistral_7b_instruct_free:
    provider: openrouter
    model_name: "mistralai/mistral-7b-instruct:free"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: true
      max_tokens: 4096

  # Web search model (text-only; returns structured JSON via prompt instruction).
  or_perplexity_sonar:
    provider: openrouter
    model_name: "perplexity/sonar"
    capabilities:
      mode: chat
      reasoning: false
      json_mode: false
      max_tokens: 4096

tiers:
  # High intelligence, reasoning capable
  heavy_reasoning:
    - or_deepseek_v3_2_exp
    - fw_glm_4p7
    - fw_kimi_k2_thinking
    - fw_mixtral_8x22b_instruct
    - or_kimi_k2_thinking
  
  # Standard tasks
  standard:
    - or_deepseek_v3_2_exp
    - or_hunyuan_a13b_instruct
    - or_qwen_2_5_7b_instruct
    - or_gemma_3n_e2b_it_free

  # Cost effective
  cheap:
    - or_gemma_3n_e2b_it_free
    - or_hunyuan_a13b_instruct
    - or_mistral_7b_instruct_free
    - or_qwen_2_5_7b_instruct

  # Special purpose: Image generation ONLY
  image_gen:
    - gemini_2_5_flash_image

  # Special purpose: Vision captioning (image + text)
  vision_caption:
    - or_qwen_2_5_vl_72b_instruct
  
  # Special purpose: Web search (OpenRouter)
  web_search:
    - or_perplexity_sonar

  # Dedicated tier for master plan refinement (single-model only; cost guard in runner).
  master_plan_opus:
    - or_claude_opus_4_5

tasks:
  # --- Script Pipeline ---
  script_topic_research:
    tier: heavy_reasoning
    timeout: 300
    
  script_outline:
    tier: heavy_reasoning
    timeout: 180
    retry_policy: standard
    fallback_policy: tier_down

  script_chapter_brief:
    tier: heavy_reasoning
    timeout: 180

  script_chapter_draft:
    tier: heavy_reasoning
    timeout: 240
    retry_policy: extended

  script_chapter_review:
    tier: heavy_reasoning
    timeout: 180
  
  script_cta:
    tier: heavy_reasoning
    timeout: 180

  script_quality_check:
    tier: heavy_reasoning
    timeout: 300

  # --- Web Search (OpenRouter via LLMRouter) ---
  # Used by `factory_common.web_search.openrouter_web_search`.
  web_search_openrouter:
    tier: web_search
    timeout: 30
    defaults:
      temperature: 0.2
      max_tokens: 1200

  # --- Stock B-roll query normalization ---
  # Used by `video_pipeline.stock_broll.injector` to convert JP cues into short EN search queries.
  broll_query:
    tier: cheap
    timeout: 30
    defaults:
      temperature: 0.0
      max_tokens: 128

  # Master plan (blueprint) — optional Opus one-shot
  # Enable via env:
  #   SCRIPT_MASTER_PLAN_LLM=1
  #   SCRIPT_MASTER_PLAN_LLM_TASK=script_master_plan_opus
  #   SCRIPT_MASTER_PLAN_LLM_CHANNELS=CH10
  script_master_plan_opus:
    tier: master_plan_opus
    timeout: 120
    defaults:
      temperature: 0.2
      max_tokens: 1200

  # A-text seed (short draft) — default for seed-expand mode (low cost)
  script_a_text_seed:
    tier: heavy_reasoning
    timeout: 180
    defaults:
      temperature: 0.2
      max_tokens: 4096

  # A-text quality gate: Judge (diagnose) + Fixer (minimal rewrite)
  script_a_text_quality_judge:
    tier: heavy_reasoning
    timeout: 120
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 1800

  script_a_text_quality_fix:
    tier: heavy_reasoning
    timeout: 240
    defaults:
      temperature: 0.2
      max_tokens: 16384

  # A-text hard rescue: extend-only (length shortage)
  script_a_text_quality_extend:
    tier: heavy_reasoning
    timeout: 120
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 1200

  # A-text hard rescue: expand-only (large length shortage; JSON insertions)
  script_a_text_quality_expand:
    tier: heavy_reasoning
    timeout: 180
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 4096

  # A-text hard rescue: shrink-only (length excess)
  script_a_text_quality_shrink:
    tier: heavy_reasoning
    timeout: 180
    defaults:
      temperature: 0.2
      max_tokens: 16384

  # A-text rebuild (plan -> draft): last-resort for non-converging scripts
  script_a_text_rebuild_plan:
    tier: heavy_reasoning
    timeout: 120
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 1800

  script_a_text_rebuild_draft:
    tier: heavy_reasoning
    timeout: 300
    defaults:
      temperature: 0.25
      max_tokens: 16384

  # Semantic alignment: title/thumbnail promise ↔ A-text core message
  script_semantic_alignment_check:
    tier: heavy_reasoning
    timeout: 90
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 1200

  script_semantic_alignment_fix:
    tier: heavy_reasoning
    timeout: 240
    defaults:
      temperature: 0.4
      max_tokens: 16384

  script_format:
    tier: heavy_reasoning
    timeout: 60

  # Final polish (whole A-text): tone consistency + repetition control.
  # Runs at most once in script_validation (opt-in via SCRIPT_VALIDATION_FINAL_POLISH=1 or auto).
  script_a_text_final_polish:
    tier: heavy_reasoning
    timeout: 300
    defaults:
      temperature: 0.2
      max_tokens: 16384

  # --- TTS Pipeline ---
  tts_annotate:
    tier: standard
    timeout: 30
    system_prompt_override: "You are a linguistic expert."

  tts_text_prepare:
    tier: standard
    timeout: 60

  tts_segment:
    tier: standard
    timeout: 30

  tts_pause:
    tier: standard
    timeout: 30

  tts_reading:
    tier: heavy_reasoning
    timeout: 60

  tts_natural_command:
    tier: standard
    timeout: 60
    system_prompt_override: "あなたは台本編集アシスタントです。指示をJSONで返してください。"
    defaults:
      response_format: "json_object"
      temperature: 0.3
      max_tokens: 800

  # --- Visual Pipeline ---
  visual_thumbnail_caption:
    tier: vision_caption
    timeout: 60
    system_prompt_override: "あなたはYouTubeサムネイルの要約者です。短く具体的に記述してください。"
    defaults:
      temperature: 0.2
      max_tokens: 300

  # Thumbnail: user comment -> fixed param patch (layer tuning)
  thumbnail_comment_patch:
    tier: standard
    timeout: 60
    system_prompt_override: "あなたはYouTubeサムネ編集アシスタントです。必ずJSONのみ返してください。"
    defaults:
      response_format: "json_object"
      temperature: 0.2
      max_tokens: 1200

  visual_section_plan:
    tier: heavy_reasoning
    timeout: 120

  # One-shot cue planning (segments → cue ranges + visual_focus)
  # Used to keep THINK MODE to a single pending task for srt2images.
  visual_image_cues_plan:
    tier: heavy_reasoning
    timeout: 180
    defaults:
      response_format: "json_object"

  visual_persona:
    tier: heavy_reasoning
    timeout: 60

  visual_prompt_refine:
    tier: heavy_reasoning
    timeout: 60

  visual_image_gen:
    tier: image_gen
  image_generation:
    tier: image_gen
    timeout: 60

  belt_generation:
    tier: standard
    timeout: 120
    defaults:
      response_format: "json_object"

  title_generation:
    tier: standard
    timeout: 60
