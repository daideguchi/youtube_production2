# Optional task overrides for LLMRouter
# ここで task ごとに tier/models/options/system_prompt を上書きできる。
# 未設定の場合は llm_router.yaml の定義を使用する。
tasks:
  # NOTE:
  # Azure is optional. In this repo, AZURE_OPENAI_API_KEY may be intentionally disabled
  # (e.g. set to a non-ASCII placeholder), which makes Azure calls fail at the HTTP layer.
  # Prefer the default tier routing (OpenRouter) unless you explicitly want to force Azure.
  #
  # If you want to force Azure for a task, add:
  #   models: [azure_gpt5_mini]
  # BUT only when AZURE_OPENAI_API_KEY is a valid ASCII key.
  visual_bible:
    tier: heavy_reasoning
    options:
      temperature: 0.2
      response_format: json_object

  # srt2images one-shot cue planning (segments → cue ranges + visual_focus)
  # NOTE: Prefer Azure here to avoid OpenRouter credit-limit 402 loops during batch runs.
  visual_image_cues_plan:
    tier: heavy_reasoning
    models:
      - azure_gpt5_mini
    options:
      temperature: 0.3
      response_format: json_object
      max_tokens: 8000

  # A-text quality gate (script_validation): reasoning Judge + Fixer
  script_a_text_quality_judge:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_distill_qwen_32b
      - or_deepseek_r1_0528
      - or_deepseek_r1
    options:
      temperature: 0.0
      response_format: json_object
      # IMPORTANT: local llm_router config may not define per-task defaults for this task.
      # Without an explicit cap, OpenRouter can assume a very large max_tokens budget and 402 due to credits.
      max_tokens: 1800

  script_a_text_quality_fix:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_distill_qwen_32b
      - or_deepseek_r1_0528
      - or_deepseek_r1
    options:
      temperature: 0.4
      # IMPORTANT: Keep below typical OpenRouter credit-limit ceilings to avoid 402 loops.
      # Must still be large enough to output a full A-text rewrite for long-form scripts.
      max_tokens: 11000

  # A-text hard rescue: extend-only (length shortage)
  script_a_text_quality_extend:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_0528
    options:
      temperature: 0.2
      response_format: json_object
      max_tokens: 1200

  # A-text hard rescue: expand-only (large length shortage; JSON insertions)
  script_a_text_quality_expand:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_0528
    options:
      temperature: 0.2
      response_format: json_object
      max_tokens: 4096

  # A-text hard rescue: shrink-only (length excess)
  script_a_text_quality_shrink:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_0528
    options:
      temperature: 0.2
      max_tokens: 11000

  # A-text rebuild (plan -> draft): last-resort for non-converging scripts
  script_a_text_rebuild_plan:
    tier: heavy_reasoning
    models:
      - or_deepseek_r1_0528
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      response_format: json_object
      max_tokens: 1800

  script_a_text_rebuild_draft:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - or_deepseek_r1_distill_qwen_32b
      - or_deepseek_r1_0528
      - or_deepseek_r1
    options:
      temperature: 0.25
      max_tokens: 11000

  # 例: ここでは gpt-5-mini のみ利用する前提で models を固定
  # script_outline:
  #   models: [gpt-5-mini]
  #   system_prompt_override: "You are a careful editor."
  #   options:
  #     temperature: 0.2
  #     response_format: json_object
