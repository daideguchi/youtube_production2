# Optional task overrides for LLMRouter
# ここで task ごとに tier/models/options/system_prompt を上書きできる。
# 未設定の場合は llm_router.yaml の定義を使用する。

tasks:
  # NOTE:
  # - Script pipeline (台本) 系の task は Azure/GPT 系へ流さない（固定ルール）。
  # - ここでの override は、原則 Fireworks/OpenRouter のロジック名を指定する（例: or_deepseek_v3_2_exp）。
  visual_bible:
    tier: heavy_reasoning
    options:
      temperature: 0.2
      response_format: json_object

  # srt2images one-shot cue planning (segments → cue ranges + visual_focus)
  visual_image_cues_plan:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.3
      response_format: json_object
      max_tokens: 12000

  # A-text quality gate (script_validation): reasoning Judge + Fixer
  #
  # NOTE (cost/quality):
  # - DeepSeek V3.2 exp is routed via Fireworks (OpenAI-compatible). The config keeps the OpenRouter model id
  #   (`deepseek/deepseek-v3.2-exp`) as the logical name, and `packages/factory_common/llm_router.py` maps it to
  #   Fireworks model id (`accounts/fireworks/models/deepseek-v3p2`) at runtime.
  # - We enforce thinking for script-writing tasks by attaching the legacy OpenRouter-style flag:
  #     extra_body: { reasoning: { enabled: true, exclude: true } }
  #   When provider=fireworks, the router translates `reasoning.enabled` -> Fireworks `reasoning_effort` (thinking ON),
  #   and emulates `exclude: true` by extracting the final output after a marker (so chain-of-thought won't pollute output).
  #   When provider=openrouter, the router forwards it only for allowlisted models (V3.2 exp / Kimi K2 Thinking).
  # - NOTE: OpenRouter fallback is intentionally disabled for script-writing in this repo.
  #   Fireworks 側が失敗した場合は、その時点で停止（THINK MODE キューに pending を作る / runbook に従う）。

  # Planning steps: prefer V3.2(exp)+thinking; fall back to Kimi K2 Thinking.
  script_outline:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      max_tokens: 2600
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_chapter_brief:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      response_format: json_object
      temperature: 0.2
      max_tokens: 2600
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_quality_check:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      max_tokens: 2600
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # Writing steps: thinking required.
  script_topic_research:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      max_tokens: 4200
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_chapter_draft:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.25
      max_tokens: 11000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_chapter_review:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      max_tokens: 12000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_cta:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      max_tokens: 1200
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # Formatting pass (keep Fireworks-only; do not fall back to other providers).
  script_format:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.0
      max_tokens: 2000

  # Seed→Expand seed generation (one-shot short A-text).
  script_a_text_seed:
    tier: standard
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.25
      max_tokens: 4096
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # Semantic alignment (title/thumbnail ↔ script core).
  script_semantic_alignment_check:
    tier: standard
    models:
      - or_deepseek_v3_2_exp
    options:
      # NOTE: This task must return strict JSON.
      # Thinking is mandatory; we keep DeepSeek fixed and use JSON mode to avoid reasoning leakage.
      response_format: json_object
      temperature: 0.0
      max_tokens: 1600

  script_semantic_alignment_fix:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.2
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_a_text_quality_judge:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.0
      response_format: json_object
      # IMPORTANT: local llm_router config may not define per-task defaults for this task.
      # Without an explicit cap, OpenRouter can assume a very large max_tokens budget and 402 due to credits.
      max_tokens: 3600
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_a_text_quality_fix:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.25
      # IMPORTANT: Keep below typical OpenRouter credit-limit ceilings to avoid 402 loops.
      # Must still be large enough to output a full A-text rewrite for long-form scripts.
      # NOTE: 11k is too small for CH10-class long A-text; it triggers finish_reason=length and a second paid retry.
      max_tokens: 22000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # Final polish (whole A-text): tone consistency + repetition control.
  # Runs at most once in script_validation (opt-in via SCRIPT_VALIDATION_FINAL_POLISH=1 or auto).
  script_a_text_final_polish:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.2
      # NOTE: Whole-script rewrite; allow enough headroom to avoid finish_reason=length retries.
      max_tokens: 22000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # A-text hard rescue: extend-only (length shortage)
  script_a_text_quality_extend:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.2
      response_format: json_object
      # NOTE: Small JSON task, but models can still return finish_reason=length if cap is too low.
      # Keep this comfortably large to avoid failover-to-think queue.
      max_tokens: 6000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # A-text hard rescue: expand-only (large length shortage; JSON insertions)
  script_a_text_quality_expand:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.2
      response_format: json_object
      # NOTE: max_tokens は小さすぎると finish_reason=length→THINKフォールバックの原因になるので余裕を持たせる。
      max_tokens: 6000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # A-text hard rescue: shrink-only (length excess)
  script_a_text_quality_shrink:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
      - fw_glm_4p7
      - fw_kimi_k2_thinking
      - fw_mixtral_8x22b_instruct
    options:
      temperature: 0.2
      max_tokens: 11000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # A-text rebuild (plan -> draft): last-resort for non-converging scripts
  script_a_text_rebuild_plan:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.2
      response_format: json_object
      max_tokens: 1800
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  script_a_text_rebuild_draft:
    tier: heavy_reasoning
    models:
      - or_deepseek_v3_2_exp
    options:
      temperature: 0.25
      max_tokens: 11000
      extra_body:
        reasoning:
          enabled: true
          exclude: true

  # 例: 特定taskの models を固定する
  # script_outline:
  #   models: [or_deepseek_v3_2_exp]
  #   system_prompt_override: "You are a careful editor."
  #   options:
  #     temperature: 0.2
  #     response_format: json_object
