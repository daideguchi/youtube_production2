from tools.script_validation_handler import handle_script_validation
#!/usr/bin/env python3
"""
processing_status.json ã¨ Google Sheets ã®é€²æ—ã‚’çµ±ä¸€çš„ã«ç®¡ç†ã™ã‚‹CLI

Usage examples:
  # é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–ï¼ˆå°æœ¬åˆ¶ä½œé–‹å§‹æ™‚ï¼‰
  python3 tools/progress_manager.py init --channel-code CH08 --video-number 3

  # ã‚¹ãƒ†ãƒ¼ã‚¸æ›´æ–°ï¼ˆã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³å®Œäº†ï¼‰
  python3 tools/progress_manager.py update-stage \
      --channel-code CH08 \
      --video-number 3 \
      --stage script_outline \
      --state completed \
      --detail agent=Qwen \
      --update-sheet \
      --row-key 3

  # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
  python3 tools/progress_manager.py set-status \
      --channel-code CH08 \
      --video-number 3 \
      --status script_validated \
      --update-sheet \
      --row-key 3

  # ç¾åœ¨ã®é€²æ—ã‚’è¡¨ç¤º
  python3 tools/progress_manager.py show --channel-code CH08 --video-number 3

  # ãƒãƒ£ãƒ³ãƒãƒ«å…¨ä½“ã®é€²æ—ä¸€è¦§
  python3 tools/progress_manager.py summary --channel-code CH08
"""

from __future__ import annotations

import argparse
import json
import sys
import shutil
import subprocess
from pathlib import Path
from collections import OrderedDict
from dataclasses import dataclass
from datetime import datetime
from textwrap import indent
from typing import Dict, Any, Optional, List, Tuple, Set
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from app import sheets_io, progress_tracker  # noqa: E402

DEFAULT_PENDING_FILE = PROJECT_ROOT / "tasks" / "pending_scripts.md"


STAGE_METADATA: "OrderedDict[str, Dict[str, str]]" = OrderedDict(
    [
        (
            "kimi_research",
            {
                "title": "Kimiãƒªã‚µãƒ¼ãƒçµ±åˆ",
                "owner": "Kimi K2 / Brave Search",
                "criteria": "analysis/kimi/research_brief.md ã¨ references.json ã‚’æ›´æ–°ã—ã€æ¬¡å·¥ç¨‹ã§å‚ç…§å¯èƒ½ãªè¦ç‚¹ã‚’æ•´ç†",
            },
        ),
        (
            "script_outline",
            {
                "title": "ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³è¨­è¨ˆ",
                "owner": "å°æœ¬æ‹…å½“ï¼ˆLLM / Qwenï¼‰",
                "criteria": "outline.md ãŒå®Œæˆã— CLI ã§ completed ã«æ›´æ–°",
            },
        ),
        (
            "script_draft",
            {
                "title": "ç« ã”ã¨ã®è‰ç¨¿",
                "owner": "å°æœ¬æ‹…å½“ï¼ˆLLM / Qwenï¼‰",
                "criteria": "section_0*.md ãŒæƒã„å†…å®¹èª¿æ•´ãŒå®Œäº†",
            },
        ),
        (
            "script_review",
            {
                "title": "é€£çµï¼†ãƒªãƒ©ã‚¤ãƒˆ",
                "owner": "å°æœ¬æ‹…å½“ / ãƒ¬ãƒ“ãƒ¥ã‚¢",
                "criteria": "assembled.md ãŒæ•´å½¢æ¸ˆã¿ã€Summary è¿½è¨˜æ¸ˆã¿",
            },
        ),
        (
            "kimi_quality_review",
            {
                "title": "Kimiå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼",
                "owner": "Kimi K2",
                "criteria": "analysis/kimi/quality_review.md ã«ãƒˆãƒ¼ãƒ³èª¿æ•´ãƒ»èª¬å¾—åŠ›å¼·åŒ–ãƒ»å·®åˆ†å¯¾å¿œã®ææ¡ˆã¨åæ˜ ãƒã‚§ãƒƒã‚¯ã‚’è¨˜éŒ²",
            },
        ),
        (
            "script_validation",
            {
                "title": "ãƒ­ãƒ¼ã‚«ãƒ«æ¤œè¨¼",
                "owner": "å°æœ¬æ‹…å½“",
                "criteria": "validate_local_script.py ãŒ SUCCESSã€validation.log è¨˜éŒ²æ¸ˆã¿",
            },
        ),
        (
            "audio_synthesis",
            {
                "title": "éŸ³å£°åˆæˆ",
                "owner": "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
                "criteria": "app.pipeline å®Ÿè¡Œã§è‡ªå‹•çš„ã«æ›´æ–°ã•ã‚Œã‚‹ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ WAV å®Œäº†ï¼‰",
            },
        ),
        (
            "srt_generation",
            {
                "title": "å­—å¹•ç”Ÿæˆ",
                "owner": "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
                "criteria": "æœ€çµ‚ SRT ãŒç”Ÿæˆã•ã‚Œã€verify_srt_sync æˆåŠŸ",
            },
        ),
        (
            "timeline_copy",
            {
                "title": "Timeline é€£æº",
                "owner": "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
                "criteria": "srt2images-timeline/input/ ã¸ã‚³ãƒ”ãƒ¼æ¸ˆã¿",
            },
        ),
        (
            "image_generation",
            {
                "title": "ç”»åƒç”Ÿæˆ",
                "owner": "srt2images-timeline ç­‰",
                "criteria": "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”»åƒãŒç”Ÿæˆã•ã‚Œ image_count ãŒæ›´æ–°",
            },
        ),
        (
            "capcut_draft",
            {
                "title": "CapCutãƒ‰ãƒ©ãƒ•ãƒˆ",
                "owner": "å‹•ç”»ç·¨é›†æ‹…å½“",
                "criteria": "CapCut ã§ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆã— draft_name ã‚’è¨˜éŒ²",
            },
        ),
    ]
)


STATE_ICONS = {
    progress_tracker.ProgressStatus.STAGE_COMPLETED: "âœ…",
    progress_tracker.ProgressStatus.STAGE_PROCESSING: "ğŸ”„",
    progress_tracker.ProgressStatus.STAGE_FAILED: "âŒ",
    progress_tracker.ProgressStatus.STAGE_PENDING: "ğŸ•—",
    # è¿½åŠ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ - ã‚„ã‚Šç›´ã—å¯¾å¿œ
    progress_tracker.ProgressStatus.STAGE_RERUN_REQUESTED: "ğŸ”„âš ï¸",
    progress_tracker.ProgressStatus.STAGE_RERUN_IN_PROGRESS: "ğŸ”„ğŸ“",
    progress_tracker.ProgressStatus.STAGE_RERUN_COMPLETED: "ğŸ”„ğŸ",
}

GLOBAL_STATUS_ICONS = {
    progress_tracker.ProgressStatus.SCRIPT_IN_PROGRESS: "ğŸ“",
    progress_tracker.ProgressStatus.SCRIPT_READY: "ğŸ“„",
    progress_tracker.ProgressStatus.SCRIPT_VALIDATED: "ğŸ§ª",
    progress_tracker.ProgressStatus.PROCESSING: "ğŸ”„",
    progress_tracker.ProgressStatus.AUDIO_DONE: "ğŸ”Š",
    progress_tracker.ProgressStatus.TIMELINE_READY: "ğŸ¬",
    progress_tracker.ProgressStatus.IMAGES_DONE: "ğŸ–¼ï¸",
    progress_tracker.ProgressStatus.CAPCUT_DONE: "âœ‚ï¸",
    progress_tracker.ProgressStatus.COMPLETED: "ğŸ",
    progress_tracker.ProgressStatus.PENDING: "ğŸ•—",
    # è¿½åŠ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ - ã‚„ã‚Šç›´ã—å¯¾å¿œ
    progress_tracker.ProgressStatus.RERUN_REQUESTED: "ğŸ”„âš ï¸",
    progress_tracker.ProgressStatus.RERUN_IN_PROGRESS: "ğŸ”„ğŸ“",
    progress_tracker.ProgressStatus.RERUN_COMPLETED: "ğŸ”„ğŸ",
}


def sanitize_component(value: Optional[str]) -> str:
    if not value:
        return "unknown"
    sanitized = value.replace("/", "_").replace("\\", "_")
    sanitized = sanitized.replace(" ", "_")
    return sanitized


def load_channel_configs(config_path: Path) -> Dict[str, Dict[str, Any]]:
    with config_path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    return {item["code"]: item for item in data}


def resolve_channel_name(
    channels: Dict[str, Dict[str, Any]], channel_code: str, explicit_name: Optional[str]
) -> str:
    if explicit_name:
        return explicit_name
    channel_info = channels.get(channel_code)
    if not channel_info:
        raise ValueError(f"ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ '{channel_code}' ãŒ configs/sheets.json ã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    return channel_info.get("worksheet") or channel_info.get("name")


def resolve_storage_channel_name(
    channels: Dict[str, Dict[str, Any]], channel_code: str, fallback: Optional[str]
) -> str:
    channel_info = channels.get(channel_code)
    if not channel_info:
        return fallback or channel_code
    return channel_info.get("name") or channel_info.get("display_name") or fallback or channel_code


def resolve_project_path(path_str: str) -> Path:
    candidate = Path(path_str)
    if not candidate.is_absolute():
        candidate = (PROJECT_ROOT / candidate).resolve()
    return candidate


def copy_if_exists(
    src_str: Optional[str],
    dest_dir: Path,
    *,
    dest_name: Optional[str] = None,
    label: str = "",
) -> Optional[Path]:
    if not src_str:
        return None
    candidate = resolve_project_path(src_str)
    if not candidate.exists():
        logger.warning(f"ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚³ãƒ”ãƒ¼å¤±æ•—: {candidate} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ ({label})")
        return None
    dest_dir.mkdir(parents=True, exist_ok=True)
    target = dest_dir / (dest_name or candidate.name)
    try:
        shutil.copy2(candidate, target)
        logger.info(f"ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜: {label or target.name} -> {target}")
        return target
    except Exception as exc:  # noqa: BLE001
        logger.error(f"ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {candidate} -> {target} ({exc})")
        return None


def normalise_video_number(raw: str) -> str:
    raw = str(raw).strip()
    if raw.isdigit():
        return f"{int(raw):03d}"
    safe = raw.replace("/", "_").replace(" ", "_")
    return safe or "000"


def determine_script_id(channel_code: str, video_token: str, explicit: Optional[str]) -> str:
    if explicit:
        return explicit
    if channel_code and video_token:
        return f"{channel_code}-{video_token}"
    return video_token or channel_code


def determine_script_title_from_assembled(assembled_path: Path) -> str:
    """
    assembled.mdãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã¾ãŸã¯ç”Ÿæˆã™ã‚‹
    å®Ÿéš›ã«ã¯ã€tools/generate_script_filename.pyã‚’ä½¿ç”¨ã™ã¹ãã ãŒã€
    ä¾¿å®œä¸Šç°¡æ˜“çš„ãªå®Ÿè£…ã‚’è¡Œã†
    """
    if not assembled_path.exists():
        return "å°æœ¬"
    
    try:
        content = assembled_path.read_text(encoding="utf-8")
        lines = content.splitlines()
        
        # æœ€åˆã®æœ‰æ„ç¾©ãªè¡Œã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼ˆè¦‹å‡ºã—ã€å¤ªå­—ã€ã¾ãŸã¯æ™®é€šã®æ–‡ç« ï¼‰
        for line in lines:
            line = line.strip()
            # è¦‹å‡ºã—è¡Œã‚’ãƒã‚§ãƒƒã‚¯ (# ã§å§‹ã¾ã‚‹)
            if line.startswith("#"):
                title = line.lstrip("# ").strip()
                # é•·ã™ããŸã‚‰çœç•¥
                if len(title) > 50:
                    title = title[:50] + "..."
                return sanitize_title(title)
            
            # å¤ªå­—ï¼ˆ**text**ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
            import re
            bold_match = re.search(r'\*\*(.+?)\*\*', line)
            if bold_match:
                title = bold_match.group(1).strip()
                if len(title) > 50:
                    title = title[:50] + "..."
                return sanitize_title(title)
        
        # è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return "å°æœ¬"
    except Exception as exc:
        logger.warning(f"assembled.mdã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã«å¤±æ•—: {exc}")
        return "å°æœ¬"


def sanitize_title(title: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã™ã‚‹
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆãªã„æ–‡å­—ã‚’ç½®æ›
    invalid_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        title = title.replace(char, '_')
    
    # æœ«å°¾ã®ãƒ‰ãƒƒãƒˆã‚„ç©ºç™½ã‚’å‰Šé™¤
    title = title.rstrip('. ')
    
    # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    import re
    title = re.sub(r' +', ' ', title)
    
    return title


class PendingValidationError(Exception):
    """Raised when pending_scripts.md validation fails."""


@dataclass
class PendingRow:
    channel: str
    row_key: str
    video_number: str
    title: str
    note: str
    line_index: int


def load_pending_markdown(path: Path) -> Tuple[List[PendingRow], List[str]]:
    if not path.exists():
        raise PendingValidationError(
            f"æœªä½œæˆå°æœ¬ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}. `make update-pending` ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    lines = path.read_text(encoding="utf-8").splitlines()
    entries: List[PendingRow] = []
    for idx, line in enumerate(lines):
        if not line.startswith("|"):
            continue
        if line.startswith("|-----------"):
            continue
        cells = [cell.strip() for cell in line.split("|")]
        if len(cells) < 6:
            continue
        channel = cells[1]
        if channel == "ãƒãƒ£ãƒ³ãƒãƒ«":
            continue
        row = PendingRow(
            channel=channel,
            row_key=cells[2],
            video_number=cells[3],
            title=cells[4],
            note=cells[5],
            line_index=idx,
        )
        entries.append(row)
    return entries, lines


def normalise_pending_token(value: str) -> str:
    if not value:
        return ""
    token = value.strip()
    if not token or token == "-":
        return ""
    if token.isdigit():
        return f"{int(token):03d}"
    return token.lower()


def build_candidate_tokens(entry: PendingRow) -> Set[str]:
    tokens: Set[str] = set()
    for source in (entry.row_key, entry.video_number):
        if not source:
            continue
        raw = source.strip()
        if raw and raw != "-":
            tokens.add(raw)
        normalised = normalise_pending_token(raw)
        if normalised:
            tokens.add(normalised)
            tokens.add(f"{entry.channel}-{normalised}")
    return tokens


def update_pending_note(path: Path, lines: List[str], entry: PendingRow, note: str) -> None:
    parts = lines[entry.line_index].split("|")
    if len(parts) < 6:
        raise PendingValidationError(
            f"pending_scripts.md ã®è¡ŒãŒæƒ³å®šå¤–ã®å½¢å¼ã§ã™ (line={entry.line_index + 1})"
        )
    parts[5] = f" {note.strip()} "
    lines[entry.line_index] = "|".join(parts)
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")
    entry.note = note


def ensure_pending_claimed(
    channel_code: str,
    video_token: str,
    raw_video_number: str,
    script_id: str,
    pending_path: Path,
    claimant: Optional[str] = None,
) -> PendingRow:
    entries, lines = load_pending_markdown(pending_path)
    if not entries:
        raise PendingValidationError(
            f"{pending_path} ã«æœªä½œæˆå°æœ¬ã®è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚`make update-pending` ã§æœ€æ–°åŒ–ã—ã¦ãã ã•ã„ã€‚"
        )

    target_tokens = {
        token
        for token in [
            video_token,
            raw_video_number.strip(),
            normalise_pending_token(raw_video_number),
            script_id,
            script_id.lower(),
            f"{channel_code}-{video_token}",
            f"{channel_code}-{normalise_pending_token(raw_video_number)}",
        ]
        if token
    }

    matches: List[PendingRow] = []
    for entry in entries:
        if entry.channel != channel_code:
            continue
        entry_tokens = build_candidate_tokens(entry)
        if entry_tokens.intersection(target_tokens):
            matches.append(entry)

    if not matches:
        raise PendingValidationError(
            f"{pending_path} ã« {channel_code} / No.{raw_video_number} ã®æœªä½œæˆå°æœ¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            " `make update-pending` ã‚’å®Ÿè¡Œã—ã€è©²å½“è¡Œã‚’è¿½åŠ ã—ãŸä¸Šã§ `CLAIMED:<æ‹…å½“>@YYYY-MM-DD` ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚"
        )

    if len(matches) > 1:
        options = ", ".join(
            f"No.{entry.row_key or entry.video_number} ({entry.title})" for entry in matches
        )
        raise PendingValidationError(
            f"{channel_code} ã®å€™è£œãŒè¤‡æ•°è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {options}. `tasks/pending_scripts.md` ã‚’æ•´ç†ã—ã¦ä¸€æ„ã«ã—ã¦ãã ã•ã„ã€‚"
        )

    entry = matches[0]
    if claimant:
        stamp = datetime.now().strftime("%Y-%m-%d")
        new_note = f"CLAIMED:{claimant}@{stamp}"
        update_pending_note(pending_path, lines, entry, new_note)
        logger.info(
            f"pending_scripts.md ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {channel_code} / No.{entry.row_key or entry.video_number} -> {new_note}"
        )
    else:
        if "CLAIMED" not in entry.note.upper():
            raise PendingValidationError(
                f"{channel_code} / No.{entry.row_key or entry.video_number} ã¯ `CLAIMED:<æ‹…å½“>@YYYY-MM-DD` ãŒæœªå…¥åŠ›ã§ã™ã€‚"
                " å…ˆã« tasks/pending_scripts.md ã‚’ç·¨é›†ã—ã¦ã‹ã‚‰å†åº¦ `progress_manager.py init` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

    return entry


def parse_kv_pairs(pairs: Optional[list[str]]) -> Dict[str, Any]:
    result: Dict[str, Any] = {}
    if not pairs:
        return result
    for item in pairs:
        if "=" not in item:
            raise ValueError(f"key=value å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„: {item}")
        key, value = item.split("=", 1)
        result[key.strip()] = value.strip()
    return result


def compute_output_roots(
    output_root: Path,
    channel_code: str,
    channel_name: str,
    *,
    ensure_dirs: bool = False,
) -> Tuple[Path, Path, str]:
    code_part = sanitize_component(channel_code) if channel_code else None
    name_part = sanitize_component(channel_name)

    if code_part:
        base_name = f"{code_part}_{name_part}"
    else:
        base_name = name_part

    final_root = output_root / f"{base_name}_final"
    temp_root = output_root / f"{base_name}_temp"

    if ensure_dirs:
        final_root.mkdir(parents=True, exist_ok=True)
        temp_root.mkdir(parents=True, exist_ok=True)

    return final_root, temp_root, base_name


def build_sheet_message(stage: str, state: str, details: Dict[str, Any]) -> str:
    str_details = {k: str(v) for k, v in details.items()}
    return progress_tracker.ProgressTracker.format_sheet_status(stage, state, str_details)


def ensure_tracker(
    output_root: Path,
    channel_code: str,
    video_number: str,
    script_id: str,
    channel_name: str,
    channels: Dict[str, Dict[str, Any]],
    metadata: Optional[Dict[str, Any]] = None,
    *,
    create: bool = True,
) -> progress_tracker.ProgressTracker:
    final_root, temp_root, _ = compute_output_roots(
        output_root, channel_code, channel_name, ensure_dirs=create
    )
    if create:
        temp_root.mkdir(parents=True, exist_ok=True)
    output_dir = final_root / video_number
    tracker = progress_tracker.ProgressTracker(output_dir)
    if create:
        tracker.initialize(script_id, channel_name, metadata=metadata)
    return tracker


def update_sheet_progress(
    worksheet_name: str, row_key: str, message: str
) -> None:
    if not row_key:
        raise ValueError("ã‚·ãƒ¼ãƒˆæ›´æ–°ã«ã¯ --row-key ãŒå¿…è¦ã§ã™")
    sheets_io.update_progress(worksheet_name, row_key, message)


def icon_for_state(state: str) -> str:
    return GLOBAL_STATUS_ICONS.get(
        state,
        STATE_ICONS.get(state, "â€¢"),
    )


def stage_details_lines(stage_name: str, stage_data: Dict[str, Any]) -> List[str]:
    lines: List[str] = []
    meta = STAGE_METADATA.get(stage_name)
    if meta:
        lines.append(f"owner: {meta['owner']}")
        lines.append(f"criteria: {meta['criteria']}")
    for key, value in stage_data.items():
        if key in {"status"}:
            continue
        lines.append(f"{key}: {value}")
    return lines


def render_status_report(status: Dict[str, Any]) -> str:
    lines: List[str] = []
    script_id = status.get("script_id", "N/A")
    channel = status.get("channel", "N/A")
    overall_status = status.get("status", progress_tracker.ProgressStatus.PENDING)
    updated_at = status.get("updated_at", "N/A")

    icon = GLOBAL_STATUS_ICONS.get(overall_status, "â€¢")
    lines.append(f"Script ID : {script_id}")
    lines.append(f"Channel   : {channel}")
    lines.append(f"Status    : {icon} {overall_status}")
    lines.append(f"Updated   : {updated_at}")

    metadata = status.get("metadata")
    if metadata:
        lines.append("Metadata :")
        for key, value in metadata.items():
            lines.append(f"  - {key}: {value}")

    stages = status.get("stages", {})
    lines.append("Stages:")
    next_stage_name: Optional[str] = None
    for stage_name, meta in STAGE_METADATA.items():
        stage_data = stages.get(stage_name, {})
        state = stage_data.get(
            "status", progress_tracker.ProgressStatus.STAGE_PENDING
        )
        if next_stage_name is None and state != progress_tracker.ProgressStatus.STAGE_COMPLETED:
            next_stage_name = stage_name
        icon = STATE_ICONS.get(state, "â€¢")
        title = meta["title"]
        lines.append(f"  {icon} {stage_name} : {title} [{state}]")
        detail_lines = stage_details_lines(stage_name, stage_data)
        for detail in detail_lines:
            lines.append(f"      - {detail}")

    # æœªå®šç¾©ã‚¹ãƒ†ãƒ¼ã‚¸ (å°†æ¥æ‹¡å¼µ)
    for stage_name, stage_data in stages.items():
        if stage_name in STAGE_METADATA:
            continue
        state = stage_data.get(
            "status", progress_tracker.ProgressStatus.STAGE_PENDING
        )
        icon = STATE_ICONS.get(state, "â€¢")
        lines.append(f"  {icon} {stage_name} : (custom) [{state}]")
        for detail in stage_details_lines(stage_name, stage_data):
            lines.append(f"      - {detail}")

    if next_stage_name:
        next_meta = STAGE_METADATA.get(next_stage_name)
        if next_meta:
            lines.append("")
            lines.append(
                f"Next action â†’ {next_stage_name}: {next_meta['title']} "
                f"(owner: {next_meta['owner']})"
            )

    return "\n".join(lines)


def summarize_channel(
    output_root: Path,
    channels: Dict[str, Dict[str, Any]],
    channel_code: str,
    channel_name: str,
) -> Tuple[str, List[Tuple[str, str, Path]]]:
    final_root, _, base_name = compute_output_roots(
        output_root, channel_code, channel_name, ensure_dirs=False
    )
    items: List[Tuple[str, str, Path]] = []
    if not final_root.exists():
        return base_name, items

    for entry in sorted(final_root.iterdir()):
        if not entry.is_dir():
            continue
        status_path = entry / progress_tracker.ProgressTracker.STATUS_FILE
        if status_path.exists():
            try:
                data = json.loads(status_path.read_text(encoding="utf-8"))
                state = data.get("status", progress_tracker.ProgressStatus.PENDING)
            except Exception:
                state = "invalid"
        else:
            state = "missing"
        items.append((entry.name, state, status_path))
    return base_name, items


def cmd_init(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    sheet_name = resolve_channel_name(channels, args.channel_code, args.channel_name)
    storage_name = resolve_storage_channel_name(channels, args.channel_code, sheet_name)
    video_token = normalise_video_number(args.video_number)
    script_id = determine_script_id(args.channel_code, video_token, args.script_id)
    metadata = parse_kv_pairs(args.meta)

    pending_path = Path(args.pending_file).resolve()
    try:
        ensure_pending_claimed(
            args.channel_code,
            video_token,
            args.video_number,
            script_id,
            pending_path,
            claimant=args.claimant,
        )
    except PendingValidationError as exc:
        logger.error(exc)
        sys.exit(1)

    work_root = Path("work") / args.channel_code.lower() / video_token
    kimi_dir = work_root / "analysis" / "kimi"
    kimi_dir.mkdir(parents=True, exist_ok=True)

    research_path = kimi_dir / "research_brief.md"
    if not research_path.exists():
        research_path.write_text(
            "# Kimiãƒªã‚µãƒ¼ãƒãƒ–ãƒªãƒ¼ãƒ•\n\n"
            "## ã‚´ãƒ¼ãƒ«\n"
            "- è¦–è´è€…åƒãƒ»èª²é¡Œæ„Ÿ\n"
            "- ä»Šå›ã®å°æœ¬ã§å±Šã‘ã‚‹æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n"
            "- Kimi K2 / Brave Search ã§å¾—ãŸè¦ç‚¹ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n"
            "\n"
            "## ãƒ¡ãƒ¢\n"
            "- æ¯”è¼ƒã™ã¹ãè¦–ç‚¹ / åè«–ã®èŠ½\n"
            "- å‚ç…§ã—ãŸã‚¯ã‚¨ãƒªã¨çµæœã‚µãƒãƒª\n",
            encoding="utf-8",
        )

    references_path = kimi_dir / "references.json"
    if not references_path.exists():
        references_path.write_text(
            json.dumps({"sources": []}, ensure_ascii=False, indent=2) + "\n",
            encoding="utf-8",
        )

    quality_path = kimi_dir / "quality_review.md"
    if not quality_path.exists():
        quality_path.write_text(
            "# Kimiå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼\n\n"
            "## ãƒã‚§ãƒƒã‚¯è¦³ç‚¹\n"
            "- ãƒˆãƒ¼ãƒ³ã¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ä¸€è²«æ€§\n"
            "- æ„Ÿæƒ…å°ç·šã¨èª¬å¾—åŠ›ã‚’é«˜ã‚ã‚‹è¿½åŠ æ¡ˆ\n"
            "- ãƒ‡ãƒ¼ã‚¿ãƒ»äº‹å®Ÿç¢ºèªã®æŒ‡æ‘˜\n"
            "\n"
            "## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n"
            "- [ ] ç« åˆ¥ã‚³ãƒ¡ãƒ³ãƒˆ\n"
            "- [ ] è¿½åŠ ã§ç››ã‚Šè¾¼ã‚€ã¹ãæƒ…å ±\n",
            encoding="utf-8",
        )

    tracker = ensure_tracker(
        Path(args.output_root),
        args.channel_code,
        video_token,
        script_id,
        storage_name,
        channels,
        metadata=metadata or None,
    )
    logger.success(
        f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–: {tracker.status_file} "
        f"(script_id={script_id}, channel={storage_name})"
    )


def cmd_update_stage(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    # ç’°å¢ƒå¤‰æ•°QWEN_STRICTã‚’ç¢ºèªã—ã€--strictãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    import os
    if not hasattr(args, 'strict') or args.strict is None:
        args.strict = os.environ.get('QWEN_STRICT', '1') != '0'
    
    sheet_name = resolve_channel_name(channels, args.channel_code, args.channel_name)
    storage_name = resolve_storage_channel_name(channels, args.channel_code, sheet_name)
    video_token = normalise_video_number(args.video_number)
    script_id = determine_script_id(args.channel_code, video_token, args.script_id)
    metadata = parse_kv_pairs(args.meta)
    details = parse_kv_pairs(args.detail)

    tracker = ensure_tracker(
        Path(args.output_root),
        args.channel_code,
        video_token,
        script_id,
        storage_name,
        channels,
        metadata=metadata or None,
    )

    artifact_details = details.copy() if details else {}

    work_root = Path("work") / args.channel_code.lower() / video_token
            # script_validationã‚¹ãƒ†ãƒ¼ã‚¸ã®å‡¦ç†ã‚’handle_script_validationé–¢æ•°ã«å§”è­²
handle_script_validation(args, artifact_details, work_root, temp_dir, script_id, video_token, storage_name)

    # strictãƒ¢ãƒ¼ãƒ‰ã®ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ã‚’å…ˆã«è¡Œã†
    elif args.strict and args.stage == "script_draft" and args.state == "completed":
        # script_draftã®å ´åˆã¯ã€ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã¨ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ãŒå¿…è¦
        work_dir = Path(f"work/{args.channel_code.lower()}/{video_token}")
        chapters_dir = work_dir / "chapters"
        outline_path = work_dir / "outline.md"
        
        missing_items = []
        if not chapters_dir.exists():
            missing_items.append(f"chapters directory: {chapters_dir}")
        if not outline_path.exists():
            missing_items.append(f"outline file: {outline_path}")
            
        if missing_items:
            logger.error(f"strictãƒ¢ãƒ¼ãƒ‰: å¿…é ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_items}")
            raise FileNotFoundError(f"å¿…é ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_items}")

    tracker.update_stage(
        args.stage,
        args.state,
        details=details if details else None,
        update_global_status=args.global_status,
    )

    output_root_path = Path(args.output_root)
    final_root, temp_root, _ = compute_output_roots(
        output_root_path, args.channel_code, storage_name, ensure_dirs=True
    )
    temp_dir = temp_root / video_token
    final_dir = final_root / video_token
    temp_dir.mkdir(parents=True, exist_ok=True)
    final_dir.mkdir(parents=True, exist_ok=True)

    if args.stage == "script_review":
        assembled_path = artifact_details.get("assembled_path")
        copy_if_exists(assembled_path, temp_dir, dest_name="assembled.md", label="assembled")
        gpt_feedback = artifact_details.get("gpt5_feedback") or artifact_details.get("feedback_path")
        copy_if_exists(gpt_feedback, temp_dir, dest_name="gpt5_feedback.json", label="gpt5_feedback")
    elif args.stage == "kimi_research":
        kimi_dest = temp_dir / "analysis" / "kimi"
        research_path = artifact_details.get("research_path") or str(kimi_workspace / "research_brief.md")
        references_path = artifact_details.get("references_path") or str(kimi_workspace / "references.json")
        copy_if_exists(research_path, kimi_dest, dest_name="research_brief.md", label="kimi_research")
        copy_if_exists(references_path, kimi_dest, dest_name="references.json", label="kimi_research_refs")
    elif args.stage == "kimi_quality_review":
        kimi_dest = temp_dir / "analysis" / "kimi"
        review_path = artifact_details.get("quality_path") or str(kimi_workspace / "quality_review.md")
        copy_if_exists(review_path, kimi_dest, dest_name="quality_review.md", label="kimi_quality_review")
    elif args.stage == "script_draft" and args.state == "completed":
        # script_draftãŒcompletedã«ãªã‚‹å‰ã«ã€ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã®æ–‡å­—æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        # work_dir ã¯ output/CHXX_XXX_final/NNN ã¨ã„ã†å½¢å¼
        # chapters_dir ã¯ work_dir / "chapters"
        # outline.md ã¯ work_dir / "outline.md"
        work_dir = tracker.status_file.parent
        chapters_dir = work_dir / "chapters"
        outline_path = work_dir / "outline.md"
        
        if not chapters_dir.exists():
            logger.error(f"Chapters directory not found: {chapters_dir}")
            raise ValueError(f"Chapters directory not found: {chapters_dir}")
            
        if not outline_path.exists():
            logger.error(f"Outline file not found: {outline_path}")
            raise ValueError(f"Outline file not found: {outline_path}")
            
        # ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã®æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        check_cmd = [
            sys.executable, "-m", "tools.check_and_enhance_chapter_length",
            "--outline", str(outline_path),
            "--chapters-dir", str(chapters_dir)
        ]
        
        logger.info(f"Running chapter length check: {' '.join(check_cmd)}")
        result = subprocess.run(check_cmd, cwd=PROJECT_ROOT, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"Chapter length check failed: {result.stderr}")
            raise ValueError(f"Chapter length check failed. Please revise chapters to meet word targets.\n{result.stdout}\n{result.stderr}")
            
        logger.success("All chapters passed length check.")
        
        # ãƒã‚§ãƒƒã‚¯é€šéã‚’é€²æ—ã«è¨˜éŒ²
        tracker.update_stage(
            args.stage,
            args.state,
            details={"length_check_passed": True},
            update_global_status=args.global_status,
        )
        # æ—¢ã«tracker.update_stageã‚’å®Ÿè¡Œã—ãŸã®ã§ã€å¾Œã®å…±é€šå‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—
        if args.update_sheet:
            row_key = args.row_key or script_id
            if isinstance(row_key, str) and row_key.isdigit():
                row_key_lookup = int(row_key)
            else:
                row_key_lookup = row_key
            channel_info = channels.get(args.channel_code, {})
            worksheet_name = resolve_channel_name(
                channels, args.channel_code, channel_info.get("worksheet")
            )
            sheet_details = details.copy()
            sheet_details.update(parse_kv_pairs(args.sheet_extra))
            sheet_message = build_sheet_message(args.stage, args.state, sheet_details)
            update_sheet_progress(worksheet_name, row_key_lookup, sheet_message)
            logger.success(f"ã‚·ãƒ¼ãƒˆé€²æ—æ›´æ–°: worksheet={worksheet_name}, row={row_key}")
        return  # å‡¦ç†ã‚’çµ‚äº†
                except Exception as e:
                    logger.warning(f"generate_script_filename ã«å¤±æ•—: {e}")

            # 3) æœ€å¾Œã«ä¸€ç™ºã§ç¢ºå®šï¼ˆåˆ†å²æ¼ã‚Œã‚’çµ¶å¯¾ã«ä½œã‚‰ãªã„ï¼‰
            final_script_name = f"{script_id}_{script_title}.txt"
            # ä»¥é™ã€ã“ã® final_script_name ã‚’ä½¿ã£ã¦é…ç½®ãƒ»ã‚³ãƒ”ãƒ¼
            
            final_script_path = script_dir / final_script_name
            
            # æ•´å½¢æ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼
            source_final_path = resolve_project_path(final_path)
            if source_final_path.exists():
                shutil.copy2(source_final_path, final_script_path)
                logger.info(f"æ•´å½¢æ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿å­˜: {final_script_path}")
            else:
                logger.warning(f"æ•´å½¢æ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“: {source_final_path}")
        
        # åŸæ–‡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ(assembled.md)ã®å‡¦ç†
        assembled_path = work_root / "assembled.md"
        if assembled_path.exists():
            # åŸæ–‡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’output/CHxx_XXX_temp/NNN/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            # script_titleã¯ä¸Šã§å®šç¾©æ¸ˆã¿
            original_script_name = f"{script_id}_{script_title}_åŸæ–‡.txt"
            copy_if_exists(str(assembled_path), temp_dir, dest_name=original_script_name, label="original_script")
        else:
            logger.warning(f"assembled.mdãŒå­˜åœ¨ã—ã¾ã›ã‚“: {assembled_path}")
        
        validation_log = artifact_details.get("validation_log") or artifact_details.get("validation_path")
        copy_if_exists(validation_log, temp_dir, dest_name="validation.log", label="validation_log")

    if args.update_sheet:
        row_key = args.row_key or script_id
        if isinstance(row_key, str) and row_key.isdigit():
            row_key_lookup = int(row_key)
        else:
            row_key_lookup = row_key
        channel_info = channels.get(args.channel_code, {})
        worksheet_name = resolve_channel_name(
            channels, args.channel_code, channel_info.get("worksheet")
        )
        sheet_details = details.copy()
        sheet_details.update(parse_kv_pairs(args.sheet_extra))
        sheet_message = build_sheet_message(args.stage, args.state, sheet_details)
        update_sheet_progress(worksheet_name, row_key_lookup, sheet_message)
        logger.success(f"ã‚·ãƒ¼ãƒˆé€²æ—æ›´æ–°: worksheet={worksheet_name}, row={row_key}")


def cmd_set_status(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    sheet_name = resolve_channel_name(channels, args.channel_code, args.channel_name)
    storage_name = resolve_storage_channel_name(channels, args.channel_code, sheet_name)
    video_token = normalise_video_number(args.video_number)
    script_id = determine_script_id(args.channel_code, video_token, args.script_id)
    metadata = parse_kv_pairs(args.meta)

    tracker = ensure_tracker(
        Path(args.output_root),
        args.channel_code,
        video_token,
        script_id,
        storage_name,
        channels,
        metadata=metadata or None,
    )
    tracker.set_global_status(args.status)

    if args.update_sheet:
        row_key = args.row_key or script_id
        if isinstance(row_key, str) and row_key.isdigit():
            row_key_lookup = int(row_key)
        else:
            row_key_lookup = row_key
        channel_info = channels.get(args.channel_code, {})
        worksheet_name = resolve_channel_name(
            channels, args.channel_code, channel_info.get("worksheet")
        )
        details = parse_kv_pairs(args.sheet_extra)
        sheet_message = build_sheet_message("global", args.status, details)
        update_sheet_progress(worksheet_name, row_key_lookup, sheet_message)
        logger.success(f"ã‚·ãƒ¼ãƒˆé€²æ—æ›´æ–°: worksheet={worksheet_name}, row={row_key}")


def cmd_show(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    sheet_name = resolve_channel_name(channels, args.channel_code, args.channel_name)
    storage_name = resolve_storage_channel_name(channels, args.channel_code, sheet_name)
    video_token = normalise_video_number(args.video_number)
    script_id = determine_script_id(args.channel_code, video_token, args.script_id)

    tracker = ensure_tracker(
        Path(args.output_root),
        args.channel_code,
        video_token,
        script_id,
        storage_name,
        channels,
        create=False,
    )
    status = tracker.get_status()
    if args.json:
        print(json.dumps(status, ensure_ascii=False, indent=2))
    else:
        print(render_status_report(status))


def cmd_summary(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    output_root = Path(args.output_root)
    if args.channel_code:
        codes = [args.channel_code]
    else:
        codes = sorted(channels.keys())

    for code in codes:
        try:
            sheet_name = resolve_channel_name(channels, code, None)
        except ValueError:
            logger.warning(f"ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ '{code}' ã¯è¨­å®šã«å­˜åœ¨ã—ã¾ã›ã‚“")
            continue

        storage_name = resolve_storage_channel_name(channels, code, sheet_name)
        base_name, entries = summarize_channel(output_root, channels, code, storage_name)
        print(f"\n=== {code} ({storage_name}) â†’ output/{base_name}_final ===")
        if not entries:
            print("  (é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãªã—)")
            continue

        for video, state, status_path in entries:
            icon = icon_for_state(state)
            print(f"  {icon} No.{video} : {state}")
            if args.detailed and status_path.exists():
                try:
                    data = json.loads(status_path.read_text(encoding="utf-8"))
                    report = render_status_report(data)
                    report_indented = indent(report, "    ")
                    print(report_indented)
                except Exception as exc:  # noqa: BLE001
                    logger.warning(f"    ãƒ¬ãƒãƒ¼ãƒˆå–å¾—å¤±æ•— ({status_path}): {exc}")

def cmd_normalize(args: argparse.Namespace, channels: Dict[str, Dict[str, Any]]) -> None:
    output_root = Path(args.output_root)
    patched = 0
    for status_path in output_root.rglob(progress_tracker.ProgressTracker.STATUS_FILE):
        output_dir = status_path.parent
        tracker = progress_tracker.ProgressTracker(output_dir)
        data = tracker.get_status()
        if not data:
            continue
        script_id = data.get("script_id") or output_dir.name
        channel = data.get("channel") or output_dir.parent.name
        tracker.initialize(script_id, channel, metadata=data.get("metadata"))
        patched += 1
    logger.success(f"æ­£è¦åŒ–å®Œäº†: {patched}ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="çµ±ä¸€é€²æ—ç®¡ç†CLI")
    parser.add_argument("--output-root", default="output", help="å‡ºåŠ›ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument(
        "--config",
        default=str(PROJECT_ROOT / "configs" / "sheets.json"),
        help="ãƒãƒ£ãƒ³ãƒãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
    )

    subparsers = parser.add_subparsers(dest="command", required=True)

    # init
    init_parser = subparsers.add_parser("init", help="é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹")
    init_parser.add_argument("--channel-code", required=True, help="ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ (ä¾‹: CH08)")
    init_parser.add_argument("--video-number", required=True, help="å‹•ç”»ç•ªå· / è¡Œã‚­ãƒ¼")
    init_parser.add_argument("--script-id", help="script_idã‚’ç›´æ¥æŒ‡å®š (ä¾‹: CH08-003)")
    init_parser.add_argument("--channel-name", help="ã‚·ãƒ¼ãƒˆåã‚’æ˜ç¤ºçš„ã«æŒ‡å®š")
    init_parser.add_argument("--meta", action="append", help="metadataã«è¿½åŠ ã™ã‚‹key=value")
    init_parser.add_argument(
        "--claimant",
        help="CLAIMEDæ¬„ã«è¨˜éŒ²ã™ã‚‹æ‹…å½“è€…åã€‚æŒ‡å®šã™ã‚‹ã¨ pending_scripts.md ã‚’è‡ªå‹•æ›´æ–°",
    )
    init_parser.add_argument(
        "--pending-file",
        type=Path,
        default=DEFAULT_PENDING_FILE,
        help="æœªä½œæˆå°æœ¬ãƒªã‚¹ãƒˆã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tasks/pending_scripts.md)",
    )
    init_parser.set_defaults(func=cmd_init)

    # update-stage
    update_parser = subparsers.add_parser("update-stage", help="ã‚¹ãƒ†ãƒ¼ã‚¸é€²æ—ã‚’æ›´æ–°ã™ã‚‹")
    update_parser.add_argument("--channel-code", required=True)
    update_parser.add_argument("--video-number", required=True)
    update_parser.add_argument("--stage", required=True, help="ã‚¹ãƒ†ãƒ¼ã‚¸å")
    update_parser.add_argument(
        "--state",
        required=True,
        choices=[
            progress_tracker.ProgressStatus.STAGE_PENDING,
            progress_tracker.ProgressStatus.STAGE_PROCESSING,
            progress_tracker.ProgressStatus.STAGE_COMPLETED,
            progress_tracker.ProgressStatus.STAGE_FAILED,
            # è¿½åŠ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ - ã‚„ã‚Šç›´ã—å¯¾å¿œ
            progress_tracker.ProgressStatus.STAGE_RERUN_REQUESTED,
            progress_tracker.ProgressStatus.STAGE_RERUN_IN_PROGRESS,
            progress_tracker.ProgressStatus.STAGE_RERUN_COMPLETED,
        ],
        help="ã‚¹ãƒ†ãƒ¼ã‚¸ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
    )
    update_parser.add_argument("--global-status", help="å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¸ã‚‚æ›´æ–°ã™ã‚‹å ´åˆã«æŒ‡å®š")
    update_parser.add_argument("--script-id", help="script_idã‚’ç›´æ¥æŒ‡å®š")
    update_parser.add_argument("--channel-name", help="ã‚·ãƒ¼ãƒˆåã‚’æ˜ç¤ºçš„ã«æŒ‡å®š")
    update_parser.add_argument("--detail", action="append", help="ã‚¹ãƒ†ãƒ¼ã‚¸è©³ç´° key=value")
    update_parser.add_argument("--meta", action="append", help="metadataã«è¿½åŠ ã™ã‚‹key=value")
    update_parser.add_argument("--update-sheet", action="store_true", help="Google Sheetsã‚‚æ›´æ–°ã™ã‚‹")
    update_parser.add_argument("--row-key", help="ã‚·ãƒ¼ãƒˆæ›´æ–°ã§ä½¿ç”¨ã™ã‚‹å°æœ¬ç•ªå·/No.")
    update_parser.add_argument("--sheet-extra", action="append", help="ã‚·ãƒ¼ãƒˆå°‚ç”¨ key=value")
    update_parser.add_argument("--strict", action="store_true", help="å¿…é ˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®å­˜åœ¨ã‚’æ¤œè¨¼ã—ã€æ¬ ã‘ã¦ã„ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã¨ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç’°å¢ƒå¤‰æ•°QWEN_STRICTã§æ±ºå®šï¼‰")
    update_parser.set_defaults(func=cmd_update_stage)

    # set-status
    status_parser = subparsers.add_parser("set-status", help="å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°ã™ã‚‹")
    status_parser.add_argument("--channel-code", required=True)
    status_parser.add_argument("--video-number", required=True)
    status_parser.add_argument(
        "--status",
        required=True,
        choices=[
            progress_tracker.ProgressStatus.SCRIPT_IN_PROGRESS,
            progress_tracker.ProgressStatus.SCRIPT_READY,
            progress_tracker.ProgressStatus.SCRIPT_VALIDATED,
            progress_tracker.ProgressStatus.PROCESSING,
            progress_tracker.ProgressStatus.AUDIO_DONE,
            progress_tracker.ProgressStatus.TIMELINE_READY,
            progress_tracker.ProgressStatus.IMAGES_DONE,
            progress_tracker.ProgressStatus.CAPCUT_DONE,
            progress_tracker.ProgressStatus.COMPLETED,
            # è¿½åŠ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ - ã‚„ã‚Šç›´ã—å¯¾å¿œ
            progress_tracker.ProgressStatus.RERUN_REQUESTED,
            progress_tracker.ProgressStatus.RERUN_IN_PROGRESS,
            progress_tracker.ProgressStatus.RERUN_COMPLETED,
        ],
        help="å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
    )
    status_parser.add_argument("--script-id", help="script_idã‚’ç›´æ¥æŒ‡å®š")
    status_parser.add_argument("--channel-name", help="ã‚·ãƒ¼ãƒˆåã‚’æ˜ç¤ºçš„ã«æŒ‡å®š")
    status_parser.add_argument("--meta", action="append", help="metadataã«è¿½åŠ ã™ã‚‹key=value")
    status_parser.add_argument("--update-sheet", action="store_true")
    status_parser.add_argument("--row-key", help="ã‚·ãƒ¼ãƒˆæ›´æ–°ã§ä½¿ç”¨ã™ã‚‹å°æœ¬ç•ªå·/No.")
    status_parser.add_argument("--sheet-extra", action="append", help="ã‚·ãƒ¼ãƒˆå°‚ç”¨ key=value")
    status_parser.set_defaults(func=cmd_set_status)

    # show
    show_parser = subparsers.add_parser("show", help="ç¾åœ¨ã®é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹")
    show_parser.add_argument("--channel-code", required=True)
    show_parser.add_argument("--video-number", required=True)
    show_parser.add_argument("--script-id", help="script_idã‚’ç›´æ¥æŒ‡å®š")
    show_parser.add_argument("--channel-name", help="ã‚·ãƒ¼ãƒˆåã‚’æ˜ç¤ºçš„ã«æŒ‡å®š")
    show_parser.add_argument("--json", action="store_true", help="JSONå½¢å¼ã§è¡¨ç¤ºã™ã‚‹")
    show_parser.set_defaults(func=cmd_show)

    # summary
    summary_parser = subparsers.add_parser(
        "summary", help="ãƒãƒ£ãƒ³ãƒãƒ«å…¨ä½“ã®é€²æ—ä¸€è¦§ã‚’è¡¨ç¤ºã™ã‚‹"
    )
    summary_parser.add_argument("--channel-code", help="å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆçœç•¥ã§å…¨ä»¶ï¼‰")
    summary_parser.add_argument(
        "--detailed", action="store_true", help="å„å°æœ¬ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚‚è¡¨ç¤ºã™ã‚‹"
    )
    summary_parser.set_defaults(func=cmd_summary)

    # normalize
    normalize_parser = subparsers.add_parser(
        "normalize", help="outputé…ä¸‹ã®progressãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨™æº–åŒ–ã™ã‚‹"
    )
    normalize_parser.set_defaults(func=cmd_normalize)

    return parser


def main() -> int:
    parser = build_parser()
    args = parser.parse_args()

    try:
        channels = load_channel_configs(Path(args.config))
        args.func(args, channels)
        return 0
    except Exception as exc:  # noqa: BLE001
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
