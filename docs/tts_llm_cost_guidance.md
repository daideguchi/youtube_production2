# TTS向けLLMコスト削減と品質確保ガイド

## Aテキストのセクション区切り
- Aテキストは作成段階で `---` の水平線でセクションを明示し、見出しとセットで配置する。
- 水平線を基準に機械的なブロック分割を行うことで、LLMへ渡す最小単位が安定し、過剰なコンテキスト送信を防ぐ。
- 章構成が変わっても `---` が境界として機能するため、分割ロジックやポーズ挿入がシンプルになり、誤読対策の再現性が高まる。

### 現行フローのどこに組み込むか（具体指示）
- **前処理（Markdown剥がし直後）で水平線を注入する**: セクション見出しが決まった段階で `---` を埋め込む。後段の形態素解析・ポーズ生成は水平線をブロック境界としてそのまま利用する。
- **チャンク分割は水平線優先＋見出し維持で固定**: `---` を最優先の境界にし、見出しと本文を同一チャンクにまとめる。これを標準チャンク化ルールとしてLLM送信用の最小単位にする。
- **危険判定はチャンク単位で開始する**: 既存の「MeCabドラフト vs Voicevox かな一致チェック」は、水平線で分けたチャンクを単位として実施し、一致チャンクはLLMをスキップする。
- **LLM監査はチャンクごとの固定テンプレートで送信**: チャンク本文＋危険語ヒント＋規則IDだけを送る（テンプレートは後述の固定フォーマットを流用）。章全体や隣接セクションは送らない。
- **監査結果キャッシュのキーにチャンクハッシュを入れる**: `chunk_hash(original_text + dict_version)` をキーにし、水平線再配置や辞書更新以外でキャッシュを無効化しない。

## トークンコストを下げつつ品質を維持するための施策
### 1. 事前分割と選択的送信
- `---` 区切りや見出し単位でAテキストを先にチャンク化し、危険語/外来語/数字混在のチャンクのみLLMに渡す。
- MeCabやSudachiでの標準読みとVoicevoxかなの一致確認を早段に行い、一致したチャンクはLLMをスキップ。
- **基準例**: 「10%増」「15kg」など数字＋単位の連続は必ず監査対象にする一方、「これはいい。」のような純ひらがな＋終止表現は常にスキップする。
- 警告チャンクはさらに品詞・単位パターンでフィルタリングし、「要LLM監査」ラベルを最小限にする。

### 2. プロンプトの薄型化
- システムプロンプトは共通の安全規約＋短い手順だけにし、ドメイン固有の注釈はYAML化して外部参照（テンプレートID＋差分のみ送信）。
- 例示は最小限のペア（before/after 2〜3件）に固定し、長文化した例はハッシュ化してキャッシュから復元。
- 数値や記号の読み上げ方針は「規則テーブル（例: 半角数字→漢数字不可、助数詞付きは原文）」として先送りし、プロンプトに都度列挙しない。
- **禁止具体例**: 「数字はこう読んで、単位はこう読んで…」と箇条書きで長々送るのをやめ、`reading_rules=v3` のIDだけ渡し、モデル側で紐づくテーブルを参照させる。

### 3. キャッシュと再利用
- セクションID＋ハッシュ（文章＋辞書バージョン）で監査結果をキャッシュし、再生成時は一致チャンクをそのまま流用。
- 危険語YAMLの更新時のみ「影響チャンクだけ再監査」する差分運用にし、全体再処理を避ける。

### 4. モデル粒度の切替
- 一致率の高いチャンクは軽量モデル（例: small instruct系）で監査し、不一致や外来語が多いチャンクだけ高性能モデルにフォールバック。
- フォールバック回数をログ化し、一定割合を超えたら辞書/分割ロジックの改善に投資するサイクルを回す。

### 5. 前処理の揺れ取り
- 句読点・記号の表記揺れを形態素解析前に正規化し、同一チャンクのハッシュ変化を防ぐ。
- 「GPT→ジーピーティー」のような確定置換は前処理で実行し、LLMに推測させない。

### 6. コンテキスト設計
- LLMへ渡す入力は「チャンク本文＋読み規則ID＋危険語ヒント」の3点に限定し、全体構成説明を省く。
- 章間の参照が必要な場合だけ、直前章の要約（50〜80字）を付加し、全文の再送を避ける。
- **送信テンプレート例**: `{chunk_text}\ncontext_hint: <前章80字要約 or none>\nreading_rules: v3\nhazard_terms: ["ボラタイル", "32bit"]` の固定フォーマットにし、フィールド追加を禁止する。

## Sudachi併用のヒント
- Voicevoxとの一致率向上や複合名詞の境界認識を目的に、MeCab結果とSudachi結果を比較し、差分が大きいチャンクだけLLMへ送る二段階フローが有効。
- Sudachiの`reading_form`は長音・促音が整うため、トリビアル差分の判定基準を単純化でき、LLM呼び出しのさらなる削減に繋がる。
- Sudachi用のユーザー辞書は頻出チャンネルから優先的にコンパイルし、効果を測定してから全体適用する。

## 辞書設計を「単語リスト」から進化させるための指針
単語だけの素朴なエントリでは文脈による読み分けを失いやすい。以下のような構造化と運用で、LLM依存を減らしつつ精度を底上げする。

### 1. フレーズ・文脈指向のエントリ
- **複合語・連接形で登録**: 「重複（ちょうふく）」と「重複削除（じゅうふくさくじょ）」のように、出現パターンごとにフレーズ単位で読みを固定する。
- **コンテキスト条件**: 品詞列や周辺語（例: 前が数詞/単位、後ろが助詞「で」など）を条件にして読みを切り替える。YAMLに正規表現または`pos_pattern`/`left_context`/`right_context`キーを持たせる。
- **占位子付きテンプレート**: 「〇〇版」「〇〇系」「〇〇機」はスロットを持つテンプレートで登録し、スロット部分の読みは形態素解析の結果を継承する。
- **具体的な分岐記述**: 「行為」は「悪行」など名詞連接なら「こうい」、動詞「行う」の連用形が直前なら「ぎょうい」と強制する、といったif/elseを辞書で明文化する。

### 2. 形態素属性に基づくルール
- **品詞・活用型を明示**: エントリに`pos`, `conj_type`, `conj_form`を持たせ、活用形のブレを抑える。例: サ変名詞＋「する」連結で「アクセスする→アクセススル」と読む。
- **読みの優先度とフェイルオーバー**: `priority`を付け、上位ルールが不成立なら下位の一般則（例: カタカナ長音付与）に落とす。
- **数値・単位の併記**: 「32bit」「10倍」は数字＋単位を1トークンとして読むルールを用意し、単純分割による誤読を防ぐ。
- **名詞複合の固定**: `pos_pattern: [名詞-固有名詞, 名詞-サ変接続]` のように2語連接を鍵にして「生成AI」「機械学習」を常に1単位として読む。

### 3. 周辺文脈との照合
- **前後2〜3語のN-gramで確認**: 同表記異読語は周辺N-gram一致で確定し、未一致のみLLMに送る。
- **否定・反復・引用の検知**: 「〜ではない」「『〜』」など構文パターンで読みを変える（例: 引用部は原文維持）。
- **否定時の強制戻し**: 「〜しない」「〜ではない」の直前動詞は原形読みを維持し、促音・長音の追加を抑える（例: 「見ない」は「ミナイ」で確定）。

### 4. 学習サイクルの組み込み
- **監査結果の自動取り込み**: audit_blocksの訂正結果を、チャネル別YAMLだけでなく「フレーズ辞書」「テンプレート辞書」に振り分けるスクリプトを用意する。
- **衝突検知とレビュー**: 同表記で異なる読みが登録される場合は差分レポートを出し、頻度の高い方を自動優先、低頻度は手動レビューへ回す。
- **A/B評価**: 新規辞書を影響範囲のチャンクにだけ適用する`shadow`モードを設け、LLM呼び出し削減率と誤読率の両方を記録する。
- **落とし込み例**: audit結果で「校了→コウリョウ」「校了に入る→コウリョウニハイル」が得られたら、前者を単語辞書ではなくテンプレート辞書`<base>に入る`に統合し、単語・フレーズの重複登録を避ける。

### 5. 辞書実装上のTips
- **粒度別ファイル分割**: 固有名詞、外来語、数値・単位、テンプレート、危険語などでYAMLを分割し、コンパイル時にマージする。
- **エンジン差分の吸収層**: MeCab/Sudachi双方に対応するため、内部表現は`surface`, `base`, `reading`に加え、`pos_detail`や`dictionary_form`を持たせ、どちらの出力でも照合できるキーを用意する。
- **キャッシュキーへの辞書版管理**: セクションハッシュに辞書のバージョンIDを含め、辞書更新時だけ影響チャンクを再監査する。
- **例外辞書の優先順位**: 「既知の誤読を強制修正する辞書」を最上位に置き、ユーザー辞書 → テンプレート辞書 → 汎用ルールの順に適用。コンフリクト時はログに残し、頻度とフォールバック回数で自動解決。

## 運用チェックリスト
- [ ] Aテキストに全セクションの境界として `---` を挿入済みか。
- [ ] 危険語・外来語が含まれるチャンクだけLLMへ送るフィルタが効いているか。
- [ ] 監査結果キャッシュのヒット率をモニタリングしているか。
- [ ] 軽量モデル→高性能モデルへのフォールバック頻度が閾値内か。
- [ ] 辞書更新後に影響チャンクのみ再監査できているか。
- [ ] チャンクハッシュ（原文＋辞書バージョン）をキャッシュキーに含め、レイアウト変更以外で無効化していないか。
