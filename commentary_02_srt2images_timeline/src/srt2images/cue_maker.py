from __future__ import annotations
from typing import List, Dict, Optional
import math
import logging
import os


def _truncate_summary(text: str, limit: int = 150) -> str:
    t = " ".join(text.split())
    return t if len(t) <= limit else t[: limit - 1].rstrip() + "â€¦"


def make_cues(segments: List[Dict], target_imgdur: float = 20.0, fps: int = 30, channel_id: Optional[str] = None) -> List[Dict]:
    """
    LLMæ–‡è„ˆç†è§£ã«ã‚ˆã‚‹è‡ªç„¶ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
    
    å¾“æ¥ã®æ©Ÿæ¢°çš„20ç§’åˆ†å‰²ã‚’å»ƒæ­¢ã—ã€OpenRouter LLMãŒã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æ–‡è„ˆã‚’ç†è§£ã—ã¦
    è‡ªç„¶ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œã‚’æ±ºå®šã™ã‚‹é©æ–°çš„ã‚·ã‚¹ãƒ†ãƒ 
    
    Returns cues: [{start_sec, end_sec, duration_sec, text, summary, context_reason}]
    """
    cues: List[Dict] = []
    if not segments:
        return cues

    # IMPORTANT: Mechanical splitting is forbidden.
    # If you want to stop API LLM usage, use THINK MODE failover instead of degrading quality.
    if os.getenv("SRT2IMAGES_DISABLE_CONTEXT_LLM") == "1":
        raise RuntimeError(
            "SRT2IMAGES_DISABLE_CONTEXT_LLM=1 is set, but mechanical splitting fallback is forbidden. "
            "Unset this env var and rerun (or use THINK MODE failover via the agent queue)."
        )

    # ğŸš¨ é‡è¦ï¼šLLMæ–‡è„ˆç†è§£ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
    # æ©Ÿæ¢°çš„20ç§’åˆ†å‰²ã¯å»ƒæ­¢ã•ã‚Œã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶ãªåˆ†å‰²ã‚’å®Ÿè¡Œ
    logging.info("ğŸ§  LLMæ–‡è„ˆç†è§£ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨: è‡ªç„¶ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã‚’å®Ÿè¡Œ")
    return _make_cues_with_llm_context(segments, target_imgdur, fps, channel_id=channel_id)


def _make_cues_with_llm_context(segments: List[Dict], target_imgdur: float, fps: int, channel_id: Optional[str] = None) -> List[Dict]:
    """LLMæ–‡è„ˆç†è§£ã«ã‚ˆã‚‹è‡ªç„¶ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²"""
    from .llm_context_analyzer import LLMContextAnalyzer
    from config.channel_resolver import ChannelPresetResolver
    
    try:
        # Load base_seconds from channel config (SSOT)
        base_seconds = 30.0
        
        # CH01 override: force faster pace (12s) unless manually overridden
        if (channel_id or "").upper() == "CH01":
            base_seconds = 12.0
            logging.info("âš™ï¸ CH01 detected: forcing base_seconds=%.1f for rapid pacing", base_seconds)
        elif channel_id:
            resolver = ChannelPresetResolver()
            preset = resolver.resolve(channel_id)
            if preset and preset.config_model and preset.config_model.image_generation:
                cfg_period = preset.config_model.image_generation.base_period
                if cfg_period > 0:
                    base_seconds = float(cfg_period)
                    logging.info("âš™ï¸ Configured base_seconds for %s: %.1f", channel_id, base_seconds)

        # ç·æ™‚é–“ã‹ã‚‰é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’è¨ˆç®—
        total_duration = segments[-1]["end"] - segments[0]["start"]
        target_sections = max(10, math.ceil(total_duration / base_seconds))

        # Allow environment override for desired section count (e.g. 30-40 images requirement)
        env_override = os.getenv("SRT2IMAGES_TARGET_SECTIONS")
        if env_override:
            try:
                override_val = int(env_override)
                if override_val >= 5:
                    target_sections = override_val
            except ValueError:
                logging.warning("Invalid SRT2IMAGES_TARGET_SECTIONS=%s (must be int)", env_override)
        
        logging.info("ğŸ“Š å‹•ç”»æ™‚é–“: %.1fåˆ†, ç›®æ¨™ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: %d", total_duration/60, target_sections)
        
        # LLMåˆ†æå®Ÿè¡Œ
        analyzer = LLMContextAnalyzer(channel_id=channel_id)
        section_breaks = analyzer.analyze_story_sections(segments, target_sections)
        
        cues = []
        for i, section in enumerate(section_breaks, start=1):
            slice_segments = segments[section.start_segment: section.end_segment + 1]
            if not slice_segments:
                continue
            cue = _create_context_cue(
                slice_segments,
                i,
                fps,
                context_reason=section.reason,
                emotional_tone=section.emotional_tone,
                summary_override=section.summary,
                visual_focus=section.visual_focus,
                section_type=section.section_type,
                persona_needed=section.persona_needed,
                role_tag=section.role_tag
            )
            cues.append(cue)

        # ğŸš¨ CRITICAL: é€£ç¶šæ€§ä¿è¨¼å‡¦ç†
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã«éš™é–“ãŒãªã„ã‚ˆã†ã«èª¿æ•´ï¼ˆé‡è¤‡ãªã—ãƒ»é€£ç¶šé…ç½®ï¼‰
        # æ³¨: CapCut APIã¯åŒä¸€ãƒˆãƒ©ãƒƒã‚¯ä¸Šã§ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé‡è¤‡ã‚’ç¦æ­¢

        for i in range(len(cues)):
            if i < len(cues) - 1:
                # æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ç‚¹
                next_start = cues[i+1]['start_sec']

                # ç¾åœ¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ç‚¹ã‚’æ¬¡ã®é–‹å§‹ç‚¹ã«åˆã‚ã›ã‚‹ï¼ˆéš™é–“ãªã—ã€é‡è¤‡ãªã—ï¼‰
                cues[i]['end_sec'] = next_start
                cues[i]['duration_sec'] = cues[i]['end_sec'] - cues[i]['start_sec']

            # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’å†è¨ˆç®—
            cues[i]['start_frame'] = int(round(cues[i]['start_sec'] * fps))
            cues[i]['end_frame'] = int(round(cues[i]['end_sec'] * fps))
            cues[i]['duration_frames'] = cues[i]['end_frame'] - cues[i]['start_frame']

        logging.info("âœ… LLMæ–‡è„ˆåˆ†å‰²å®Œäº†: %d ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆé€£ç¶šæ€§ä¿è¨¼ãƒ»éš™é–“ã‚¼ãƒ­ï¼‰", len(cues))
        return cues
        
    except Exception as e:
        logging.error("âŒ LLMåˆ†æå¤±æ•—: %s", e)
        raise


def _create_context_cue(
    segments: List[Dict],
    index: int,
    fps: int,
    context_reason: str = "",
    emotional_tone: str = "",
    summary_override: str | None = None,
    visual_focus: str | None = None,
    section_type: str | None = None,
    persona_needed: bool = False,
    role_tag: str | None = None,
) -> Dict:
    """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸcueä½œæˆ"""
    if not segments:
        return {}
    
    start_sec = segments[0]["start"]
    end_sec = segments[-1]["end"]
    duration_sec = end_sec - start_sec
    
    # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    all_texts = []
    for seg in segments:
        text = seg.get("text", "").strip()
        if text:
            all_texts.append(text)
    
    combined_text = " ".join(all_texts)
    summary = (summary_override or "").strip() or _truncate_summary(combined_text)

    cue = {
        "index": index,
        "start_sec": round(start_sec, 3),
        "end_sec": round(end_sec, 3),
        "duration_sec": round(duration_sec, 3),
        "text": combined_text,
        "summary": summary,
        "context_reason": context_reason,  # LLMãŒæ±ºå®šã—ãŸåˆ†å‰²ç†ç”±
        "emotional_tone": emotional_tone,  # æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³
        "start_frame": int(round(start_sec * fps)),
        "end_frame": int(round(end_sec * fps)),
        "duration_frames": max(1, int(round(end_sec * fps)) - int(round(start_sec * fps)))
    }

    if visual_focus:
        cue["visual_focus"] = visual_focus.strip()
    if section_type:
        cue["section_type"] = section_type
    if role_tag:
        cue["role_tag"] = role_tag
    # use_persona: ç‰©èª/å¯¾è©±ãªã©ã‚­ãƒ£ãƒ©ä¸€è²«ãŒå¿…è¦ãªå ´åˆã®ã¿ã‚ªãƒ³
    cue["use_persona"] = bool(persona_needed or (section_type in ("story", "dialogue")))

    return cue

